\chapter{Metodologia} % ============================================================================================== %
\section{Abordagem Computacional} % ================================================================================== %

O projeto foi desenvolvido em \textit{Python} versão 3.12. Toda as etapas, da obtenção dos dados de entrada à implementação
dos modelos computacionais foram organizadas em módulos. Todas as etapas de processamento intensivo foram realizadas em
paralelo, utilizando todas as \textit{threads} disponíveis do sistema. As etapas referentes ao modelo neural foram 
realizadas com aceleração da \textit{Graphics Processing Unit} (GPU).

Considerando que o suporte a aceleração por GPU é limitado no Windows, foi necessário realizar as etapas de treinamento do modelo 
neural em Linux, utilizando a distribuição Fedora 42. A aceleração por GPU foi necessária para reduzir o tempo de treinamento do modelo. 
O sistema utilizado possui um processador AMD Ryzen 5900X, 32 GB de memória RAM e placa de vídeo AMD RX 6800 XT. A tabela 
\ref{tab:bibliotecas} mostra as bibliotecas utilizadas no projeto, suas finalidades e versões.

\begin{table}[htb]
  \centering
   \IBGEtab{
    \caption{Bibliotecas utilizadas no projeto}
    \label{tab:bibliotecas}
    }{
    \begin{tabular}{llll}
  		  \hline
    	  \textbf{Biblioteca} & \textbf{Descrição} & \textbf{Versão} \\ \hline
        numpy & Cálculos numéricos e manipulação de arrays & 1.26.4 \\
        pandas & Manipulação e análise de dados (DataFrames) & 2.2.3 \\
        requests & Requisições HTTP & 2.32.3 \\
        urllib3 & Gerenciamento de conexões HTTP & 2.2.3 \\
        alive\_progress & Barra de progresso para loops & 3.2.0 \\
        netCDF4 & Leitura de arquivos NetCDF & 1.7.2 \\
        cdsapi & API para download de dados do ECMWF & 0.7.5 \\
        geopandas & Manipulação de dados geoespaciais & 1.0.1 \\
        matplotlib & Visualização de dados & 3.9.2 \\
        scikit-learn & Aplicação de modelos iniciais & 1.5.2 \\
        scipy & Ferramentas e algoritmos matemáticos & 1.14.1 \\
        transformers & Modelos Neurais Pré-treinadis & 4.52.3 \\
        torch & Processamento de Redes Neurais & 2.7.0 \\
        \hline
    \end{tabular}
    }{
      \fonte{o autor.}}
\end{table}

\section{Obtenção e Pré Processamento dos Dados} % =================================================================== %
\subsection{Obtenção dos Dados Energéticos e Hidrológicos} % ========================================================= %

A primeira etapa do projeto consiste na consolidação das séries históricas de geração, carga e variáveis hidrológicas,
que são disponibilizadas publicamente no portal Dados Abertos do ONS, a partir do ano 2000. As séries referentes 
às variáveis hidrológicas são disponibilizadas em base diária, e os dados de geração e carga são disponibilizados em 
base horária. As Tabelas \ref{tab:hidrologicos}, \ref{tab:carga} e \ref{tab:geracao} mostram os parâmetros dos dados hidrológicos, 
carga e geração, respectivamente.

Os dados de geração são disponibilizados em Mega Watt médio (MWmed) por fonte de energia, subsistema, estado, 
modalidade de operação, entre outras variáveis. Os dados de carga também são disponibilizados em MWmed e contêm 
informações sobre a carga em cada subsistema do SIN.

\begin{table}[htb]
  \centering
   \IBGEtab{
    \caption{Parâmetros dos dados hidrológicos}
    \label{tab:hidrologicos}
    }{
      \begin{tabular}{llll}
          \hline
          \textbf{Parâmetro} & \textbf{Descrição} & \textbf{Tipo} \\ \hline
          din\_instante & Instante de aferição & Datetime\\
          nom\_subsistema & Subsistema & String\\
          tip\_reservatorio & Tipo de reservatório & String\\
          nom\_bacia  & Bacia hidrográfica & String\\
          nom\_ree & Nome do REE & String\\
          val\_nivelmontante & Valor do nível montante (m) & Float\\
          val\_niveljusante & Valor do nível jusante (m) & Float\\
          val\_volumeutilcon & Volume útil consistido (\%) & Float\\
          val\_vazaoafluente & Vazão afluente (m³/s) & Float\\
          val\_vazaoturbinada & Vazão turbinada (m³/s) & Float\\
          val\_vazaovertida & Vazão vertida (m³/s) & Float\\
          val\_vazaodefluente & Vazão defluente (m³/s) & Float\\
          val\_vazaoevaporacaoliquida & Vazão de evaporação líquida (m³/s) & Float\\ \hline
      \end{tabular}
    } {
      \fonte{\citeonline{pen2024}}
      \nota{Variáveis não utilizadas foram omitidas.}
    }
\end{table}

Para as séries de geração, os dados de 2000 a 2021 são agrupados pelos respectivos anos, e a partir de 2022,
as informações estão agrupadas em arquivos por mês e ano. Para as séries de carga, os dados são disponibilizados por
ano. Como o ONS não disponibiliza \textit{Aplication Programming Interface} (API) para a obtenção dos dados diretamente, foi 
necessário uma outra abordagem, a fim de evitar o \textit{download} manual dos dados. 

\begin{table}[htb]
  \centering
   \IBGEtab{
    \caption{Parâmetros dos dados de carga}
    \label{tab:carga}
    }{
    \begin{tabular}{llll}
  		  \hline
    	  \textbf{Parâmetro} & \textbf{Descrição} & \textbf{Tipo} \\ \hline
        din\_instante & Instante de aferição & Datetime\\
        nom\_subsistema & Subsistema da usina & String\\
        val\_cargaenergiahomwmed & Carga de energia (MWmed) & Float\\ \hline
    \end{tabular}
    }{
      \fonte{\citeonline{pen2024}}}
\end{table}

Um script foi desenvolvido para fazer o download dos dados por meio de requisições HTTP, utilizando as bibliotecas 
\code{requests} e \code{urllib3} para gerenciar as conexões. Todos os downloads foram realizados em paralelo, utilizando 
todas as threads disponíveis do sistema. Ao todo, cerca de 10 GB de dados em arquivos Comma Separated Values (CSV) foram consolidados.

\begin{table}[htb]
  \centering
   \IBGEtab{
    \caption{Parâmetros dos dados de geração}
    \label{tab:geracao}
    }{
      \begin{tabular}{llll}
          \hline
          \textbf{Parâmetro} & \textbf{Descrição} & \textbf{Tipo} \\ \hline
          id\_subsistema & Instante de aferição & Datetime\\
          nom\_subsistema & Subsistema da usina & String\\
          id\_estado & Estado onde a usina está localizada & String\\ 
          nom\_tipousina & Tipo de usina & String\\
          nom\_tipocombustivel & Tipo de combustível & String\\
          nom\_usina & Nome da usina & String\\
          val\_geracao & Geração de energia (MWmed) & Float\\ \hline
      \end{tabular}
    } {
      \fonte{\citeonline{pen2024}}
      \nota{Variáveis não utilizadas foram omitidas.}
    }
\end{table}

Os dados de geração contém informações que permitem uma análise detalhada em diferentes níveis de granulidade. Dessa maneira, 
possíveis impactos em diferentes escalas geográficas e temporais poderão ser avaliados.

\subsection{Pré Processamento dos Dados de Geração e Carga} % ======================================================== %
Com as séries históricas de geração e carga consolidadas, se faz necessário preparar os dados para que possam ser utilizados 
nos modelos computacionais. Essa etapa envolve a verificação, transformação e limpeza dos dados. Utilizando a biblioteca
\code{pandas}, todos os arquivos com o histórico de geração e carga foram lidos e consolidados em dois \code{DataFrames} 
distintos, estrutura de dados tabulares da biblioteca. Com isso, as seguintes operações foram realizadas:
\begin{itemize}
    \item Seleção das colunas relevantes;
    \item Verificação de valores inválidos e tratamento de valores ausentes;
    \item Agrupamento dos tipos de usinas por classes: hidráulica, térmica, eólica, fotovoltaica e outras;
    \item Reamostragem para diferentes bases temporais, considerando a geração média (MWmed) e energia gerada (MWh);
    \item Agrupamento dos dados de geração por subsistema e classe.
\end{itemize}

\begin{figure}[!ht]
	\IBGEtab{\caption{Subsistemas do SIN segundo o ONS}
			 \label{fig:subsistemas_brasil}}
	{\includesvg[scale=1]{figuras/subsistemas_brasil}}
	{\fonte{o autor.}}
\end{figure}

Optou-se por fazer o agrupamento dos dados de geração por subsistema e fonte para permitir avaliar os impactos das variáveis
do ENSO em diferentes regiões do Brasil. Além disso, a representação por subsistema também é utilizada pelo modelo NEWAVE. 
A figura \ref{fig:subsistemas_brasil} mostra os subsistemas do SIN.

Diferentes arquivos CSV foram gerados, considerando a geração média (MWmed) e energia gerada (MWh) para as escalas diária e mensal.
Os dados de geração contém o histórico de geração por subsistema e fonte, enquanto que os dados de carga contém o histórico de carga
por subsistema. Considerando o período de 2000 a 2024, foram consolidados ao todo 9132 amostras diárias e 300 amostras mensais.

\subsection{Pré Processamento dos Dados Hidrológicos} % ============================================================== %
Os dados hidrológicos contém informações sobre o nível dos reservatórios e vazões. Para o contexto do projeto, espera-se que
essas informações sejam altamente correlacionados com a geração hidrelétrica, o que será verificado a seguir na seção de análise 
exploratória.

Sendo assim, os dados hidrológicos foram utilizados para verificar a performance dos modelos quando as variáveis de entrada
possuem alta correlação com a variável de saída. Dessa forma, é estabelecida uma referência para comparação entre os
resultados dos modelos.

As séries históricas foram consolidadas em um único \code{DataFrame}, contendo as vazões totais em cada subsistema do SIN,
considerando a soma das vazões de todos os reservatórios em cada subsistema, em base diária.

\subsection{Obtenção dos Dados do ENSO} % ============================================================================ %
Os dados do ENSO foram obtidos a partir do \textit{Climate Data Store} (CDS) do ECMWF. O CDS é um banco de dados com diversos
\textit{datasets} de variáveis climáticas de diferentes regiões do mundo. Para esse projeto, o dataset utilizado foi o 
\textit{ERA5 post-processed daily statistics on single levels from 1940 to present}, que contém dados diários de diversas 
variáveis, incluindo a temperatura da superfície do mar (SST).

Considerando que ENSO é um fenômeno definido pela temperatura da superfície do mar em regiões 
específicas do Oceano Pacífico, e que os dados de geração, carga e hidrológicos são disponibilizados em base diária,
optou-se por utilizar a temperatura absoluta da superfície do mar em base diária para cada uma das regiões do ENSO. As
regiões do ENSO e suas coordenadas geográficas são mostradas na Tabela \ref{tab:regioes_enso}.

\begin{table}[htb]
  \centering
   \IBGEtab{
    \caption{Regiões do ENSO}
    \label{tab:regioes_enso}
    }{
    \begin{tabular}{lll}
  		  \hline
    	  \textbf{Região} & \textbf{Latitude} & \textbf{Longitude} \\ \hline
        Niño 1+2 & -10° a 0° & -90° a -80° \\
        Niño 3 & -5° a 5° & -150° a -90° \\
        Niño 3.4 & -5° a 5° & -170° a -120° \\
        Niño 4 & -5° a 5° & -160° a -150° \\ \hline
    \end{tabular}
    }{
      \fonte{\citeonline{TheDefinitionofElNio}}}
\end{table}

Os dados foram obtidos usando a biblioteca \code{cdsapi}, que permite acessar o CDS através da
API do ECMWF. O script desenvolvido para essa etapa realiza o download dos dados de temperatura da superfície do mar
para cada uma das regiões do ENSO, considerando o período de 2000 a 2024. Os dados são obtidos em formato NetCDF, que é 
um formato de arquivo utilizado para armazenar dados científicos multidimensionais.

\subsection{Pré Processamento dos Dados do ENSO} % =================================================================== %
Após o download, os arquivos NetCDF são processados para extrair as informações relevantes para cada região do ENSO. 
Utilizando a biblioteca \code{netCDF4}, cada arquivo anual é lido para extrair as dimensões de tempo, latitude e longitude, 
além da variável de interesse, a temperatura da superfície do mar (SST).

O processamento segue as seguintes etapas:
\begin{itemize}
\item Conversão do formato das coordenadas de longitude, de 0 a 360 graus, para -180 a 180 graus;
\item Para cada uma das regiões do ENSO, um subconjunto geográfico dos dados globais é criado, selecionando os pontos
de latitude e longitude que se encontram dentro dos limites de cada região;
\item A média espacial da variável é calculada para cada dia sobre este subconjunto. Esse processo resulta em uma única série
temporal diária, que representa o valor médio da variável para aquela região específica;
\item Ao final, as séries temporais anuais de cada região são consolidadas, formando um conjunto de dados único que abrange
todo o período de análise, de 2000 a 2024.
\end{itemize}

Em resumo, o processo transforma os dados brutos multidimensionais em séries temporais diárias para cada região
do ENSO, que agora estão prontas para serem utilizadas como variáveis exógenas nos modelos computacionais.


\section{Análise Exploratória dos Dados} % =========================================================================== %
A análise exploratória dos dados é uma etapa crucial para entender as características e padrões dos dados antes de aplicar modelos computacionais.
Essa etapa envolve a visualização e análise estatística, permitindo identificar tendências, sazonalidades, correlações e possíveis outliers. 
Além das bibliotecas \code{pandas} e \code{matplotlib}, a biblioteca \code{statsmodels} foi utilizada para realizar a decomposição
das séries temporais, permitindo avaliar as sazonalidades e tendências presentes nos dados.

Para a decomposição das séries temporais, a função \code{MSTL} foi utilizada, que realiza a decomposição em componentes de 
tendência, sazonalidade e resíduo. Essa função utiliza o método de \textit{local polynomial regression fitting}, ou ajuste
de regressão polinomial local (LOESS) para decompor a série, sendo capaz de capturar sazonalidades de séries temporais
não estacionárias, que é o caso das séries de geração e carga do SIN. Para todas as análises a seguir, os dados estão
em base temporal mensal e foram normalizados utilizando o \code{StandardScaler} da biblioteca \code{scikit-learn}.

\subsection{Dados de Geração e Carga} % ============================================================================== %
\subsubsection{Dados de Geração} % =================================================================================== %
A Figura \ref{fig:decomposicao_Hidráulica} mostra a decomposição da série temporal de geração hidráulica, que revela uma 
tendência de aumento dos anos de 2000 a 2012, seguida por uma período de estabilidade de 2012 a 2024. Ao mesmo tempo,
o regime de sazonalidade apresenta maior amplitude após o ano de 2012, sugerindo uma maior variabilidade da contribuição
hidráulica para a geração do SIN. Esse comportamento pode ser explicado pela introdução em larga escala das fontes renováveis 
variáveis, como a eólica e a solar fotovoltaica, na matriz elétrica. \cite{Silva2016}
\begin{figure}[!ht]
  \IBGEtab{\caption{Decomposição da série temporal de geração hidráulica}
       \label{fig:decomposicao_Hidráulica}}
  {\includesvg[scale=1]{figuras/decomposicao_Hidráulica}}
  {\fonte{o autor.}}
\end{figure}

\begin{figure}[!ht]
  \IBGEtab{\caption{Decomposição da série temporal de geração eólica}
       \label{fig:decomposicao_Eólica}}
  {\includesvg[scale=1]{figuras/decomposicao_Eólica}}
  {\fonte{o autor.}}
\end{figure}

Conforme descrito no capítulo 1, a geração dessas fontes é intermitente e não despachável, ou seja, dependente da 
disponibilidade de vento e sol. Nesse cenário, as usinas hidrelétricas, especialmente aquelas com reservatórios, passaram 
a atuar como a principal ferramenta de flexibilidade do sistema, compensando a variabilidade das fontes intermitentes. 

Em momentos de alta geração eólica e solar, o ONS reduz a produção hidrelétrica para absorver a energia renovável, criando vales 
de geração mais profundos. Inversamente, em períodos de baixa geração renovável, como durante a noite, a geração hidrelétrica 
é acionada para suprir a demanda, resultando em picos mais elevados. Portanto, o aumento da amplitude da sazonalidade após 
2012 é a assinatura visual desse novo papel operacional, no qual a fonte hidráulica não apenas segue o ciclo hidrológico, 
mas também responde dinamicamente à intermitência e situações de pico de demanda para garantir a estabilidade do SIN. \cite{Wang2025}

A Figura \ref{fig:decomposicao_Eólica} mostra a decomposição da série temporal de geração eólica. A tendência
evidencia o crescimento exponencial da capacidade instalada a partir de 2012, reflexo das políticas de incentivo e dos leilões 
de energia. Diferentemente da fonte hidráulica, a sazonalidade da geração eólica é ditada exclusivamente 
pela disponibilidade do recurso, com picos correspondentes à "safra dos ventos", que ocorre tipicamente no segundo semestre. 

A amplitude crescente da sazonalidade e a maior variância do resíduo são consequências diretas da expansão 
da capacidade instalada. Em conjunto, a sazonalidade e o resíduo da geração eólica são as principais causas da mudança no 
perfil operacional da geração hidráulica, que ajusta sua produção para compensar a intermitência da fonte eólica.

\begin{figure}[!ht]
  \IBGEtab{\caption{Decomposição da série temporal de geração térmica}
       \label{fig:decomposicao_Térmica}}
  {\includesvg[scale=1]{figuras/decomposicao_Térmica}}
  {\fonte{o autor.}}
\end{figure}

A Figura \ref{fig:decomposicao_Térmica} apresenta a decomposição da geração térmica. A tendência atua 
como um termômetro do risco hidrológico do país, com picos proeminentes que coincidem diretamente com períodos de crise hídrica, 
como em 2014-2015 e 2021, quando o despacho térmico é massivo para garantir o suprimento. 

A sazonalidade opera de forma inversa ao ciclo hidrológico, com maior geração no período seco para poupar os reservatórios. 
O resíduo, por sua vez, exibe alta volatilidade e representa o papel da fonte térmica como recurso de ponta e de emergência, 
acionada para cobrir a intermitência de outras fontes, atender picos de demanda e responder a indisponibilidades no sistema. 


\subsubsection{Dados de Carga} % ===================================================================================== %
\begin{figure}[!ht]
  \IBGEtab{\caption{Decomposição da série temporal de carga}
       \label{fig:decomposicao_carga}}
  {\includesvg[scale=1]{figuras/decomposicao_carga}}
  {\fonte{o autor.}}
\end{figure}
A Figura \ref{fig:decomposicao_carga} mostra a decomposição da série temporal de carga. A tendência
reflete diretamente a atividade econômica do país, com um crescimento contínuo até 2014, seguido por uma estagnação durante 
a recessão de 2014-2016 e uma queda abrupta em 2020 devido à pandemia de COVID-19. \cite{Magazzino2021}

A sazonalidade é a mais regular entre todas as séries, ditada pelo clima, com picos de consumo consistentes no verão devido 
ao uso de sistemas de refrigeração. O resíduo captura variações de curto prazo, como ondas de temperatura e feriados, e 
possui uma volatilidade relativamente baixa.


\section{Implementação dos Modelos de Regressão} % =================================================================== %
Para todos os casos, os códigos referentes às implementações dos modelos estão disponíveis no Apêncice A. A seguir, será
apresentada uma visão geral das etapas de implementação, métricas de avaliação e os modelos computacionais utilizados.

\subsection{Métricas de Avaliação} % ================================================================================= %
Para todos os casos, duas métricas de avaliação foram utilizadas: o erro quadrático médio ($MSE$) e o coeficiente de determinação ($R^2$).
O $MSE$ é uma medida que quantifica a média dos erros quadráticos entre os valores reais e as previsões do modelo. O $R^2$, por outro lado, 
é uma medida que indica a proporção da variabilidade dos dados que é explicada pelo modelo, variando de 0 a 1, onde valores mais 
próximos de 1 indicam um modelo mais preciso. As equações para o cálculo dessas métricas são apresentadas a seguir:
\begin{equation}
    \label{eq:mse}
    MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\end{equation}
\begin{equation}
    \label{eq:r2}
    R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}
\end{equation}
em que $ y_i $ são os valores reais, $ \hat{y}_i $ são os valores previstos pelo modelo, $ \bar{y} $ é a média dos valores 
reais e $ n $ é o número de amostras. Essas métricas estão disponíveis na biblioteca \code{scikit-learn} através das
funções \code{mean\_squared\_error} e \code{r2\_score}, respectivamente.

\subsection{Etapas Comuns} % ========================================================================================= %
As etapas iniciais de implementação são comuns a todos os modelos, consistindo em: 
\begin{enumerate}
    \item \textbf{Entrada de dados:} Leitura dos arquivos CSV contendo os dados de geração, carga e variáveis do ENSO;
    \item \textbf{Consolidação do dataset:} Alinhamento temporal dos dados, garantindo que as séries estejam sincronizadas;
    \item \textbf{Definição das variáveis:} Seleção das clunas contendo as variáveis dependentes (geração por fonte e subsistema) e 
independentes (carga e variáveis do ENSO);
    \item \textbf{Normalização:} Normalização das variáveis independentes utilizando o \code{StandardScaler} do \code{scikit-learn};
    \item \textbf{Definição dos conjuntos para treino:} Divisão dos dados em conjuntos de treino, teste e validação (para o modelo neural);
    \item \textbf{Avaliação:} Avaliação através das métricas escolhidas e visual dos resultados através de gráficos de dispersão e linha.
\end{enumerate}

A normalização é importante para evitar que o modelo atribua maior peso às variáveis com maior amplitude,
o que poderia resultar em resultados não representativos. Para a etapa de treinamento, na qual os modelos atualizam seus parâmetros 
para minimizar o erro, os dados são divididos em conjuntos de treino, teste e validação (para o caso do modelo neural). 

O conjunto de treino é utilizado para ajustar os parâmetros 
do modelo, enquanto o conjunto de teste é utilizado para avaliar a performance do modelo em dados não vistos. Para o modelo neural, 
o conjunto de validação é utilizado para monitorar o desempenho do modelo durante o treinamento e evitar o sobreajuste. 
Diferentes tamanhos da janela de treinamento foram avaliados e serão apresentados no capítulo 5.

Os gráficos de dispersão mostram os valores reais e estimados junto a uma linha ideal, de modo que valores próximos à linha indicam 
resultados mais precisos. O outro gráfico demonstra a separação dos períodos de treino e teste, permitindo uma avaliação mais direta 
entre os resultados reais e estimados. Para facilitar a visualização, os gráficos são apresentados em base temporal mensal, 
considerando a média mensal dos valores.

\subsection{Modelo Linear} % ========================================================================================= %
O modelo \code{LinearRegression} da biblioteca \code{scikit-learn} foi implementado com a abordagem de regressão linear múltipla, 
uma técnica estatística que busca modelar a relação entre uma variável dependente e várias variáveis independentes, conforme descrito pela 
equação \ref{eq:regressao_linear}. Nesse contexto, a variável dependente é a geração de uma determinada fonte em um subsistema do 
SIN, e as variáveis independentes são as variáveis auxiliares (ou exógenas), os dados de SST e carga.

Foi considerado apenas o período de 2010 a 2024, já que a fonte eólica era pouco presente
no SIN até então. Considerando as limitações do modelo, utilizar todo o dataset poderia resultar em resultados não representativos, 
uma vez que o modelo assume que a relação entre as variáveis é linear e constante ao longo do tempo. Assim, a escolha do período 
de 2010 a 2024 é justificada pela necessidade de garantir que os dados utilizados sejam representativos do comportamento atual 
do sistema, permitindo uma análise mais precisa e confiável.

Após as etapas comuns, de carregamento de dados e tratamento inicial, os datasets de treino e teste foram definidos, de modo que
o conjunto de treino contenha 70\% dos dados e o conjunto de teste 30\%. O modelo é então instanciado com paralelismo do
CPU habilitado e treinado no conjunto de treino. A seguir, o modelo é avaliado no conjunto de teste, e os resultados são
apresentados através dos gráficos.

O modelo linear permite uma interpretação direta dos resultados, uma vez que é possível obter os coeficientes da equação
linear que descreve a relação entre as variáveis dependentes e independentes. Os demais modelos não permitem essa análise diretamente.

\subsection{Modelo Não Linear} % ===================================================================================== %
O modelo \code{RandomForestRegressor} da biblioteca \code{scikit-learn} foi implementado com a abordagem de previsão multivariada.
O modelo é um dos poucos da biblioteca que permite prever múltiplas variáveis dependentes simultaneamente, sem considerar os modelos
neurais, e foi selecionado por este motivo.

Para este caso, também foi considerado o período de 2010 a 2024, e as etapas comuns foram realizadas. Após a consolidação 
dos dados de geração, carga e ENSO, foram criados atributos temporais adicionais, como ano, mês e dia do ano, para enriquecer 
o conjunto de dados. As variáveis exógenas (carga e ENSO) foram normalizadas com o \code{StandardScaler}. O conjunto de dados 
foi então dividido cronologicamente, com 70\% dos dados para treino e 30\% para teste, utilizando a função \code{train\_test\_split} 
com o parâmetro \code{shuffle = False}.

Para definir os melhores hiperparâmetros do modelo, foi utilizado o \code{GridSearchCV}, que realiza uma busca exaustiva por 
meio de validação cruzada em grade. A validação cruzada foi adaptada para séries temporais com o uso do \code{TimeSeriesSplit}, 
que garante que os dados de treino sempre ocorram antes dos dados de validação em cada divisão. O modelo foi treinado no conjunto 
de treino, e os melhores parâmetros foram selecionados com base na métrica de erro quadrático médio ($MSE$).

Após a aplicação do \code{GridSearchCV}, o modelo foi instanciado com os parâmetros otimizados, com número de árvores de decisão 
\code{n\_estimators = 1000} e profundidade máxima \code{max\_depth = 30}, com o paralelismo habilitado \code{n\_jobs = -1}. O modelo 
treinado foi então utilizado para realizar as previsões no conjunto de teste. Os resultados foram avaliados com as métricas $R^2$ e $MSE$. 
Por fim, para cada variável alvo, foram gerados e salvos gráficos de dispersão e de série temporal para a análise visual dos resultados.


\section{Implementação do Modelo Neural} % =========================================================================== %
O modelo neural utilizado foi o \textit{TinyTimeMixer}, versão 2.1, disponível na biblioteca \code{transformers} do 
\textit{Hugging Face}. Ele é um modelo de previsão de séries temporais pré-treinado baseado na arquitetura \textit{TSMixer}, 
apresentada no capítulo 3. Modelos pré treinados são modelos que já foram treinados em grandes conjuntos de dados. Ou seja, 
os pesos das camadas de \textit{perceptrons} já foram ajustados para capturar padrões gerais em séries temporais.

Além disso, o modelo permite o processo de \textit{fine tuning}, que é a atualização dos pesos para adequar-se
a um conjunto de dados específico, considerando variáveis dependentes e independentes, que no contexto do projeto são a geração de energia
por subsistema e fonte, carga e variáveis do ENSO.

O modelo possui diferentes variações, com diferentes tamanhos de janelas de contexto e previsão e tem suporte para escalas 
de tempo semanal, diária, horária e de minutos. Para o projeto, foram utilizados as variantes com janelas de contexto e 
previsão de 512 e 96 dias, bem como a de 90 e 30 semanas.

A biblioteca \code{tsfm\_public}, que contém funções auxiliares para a implementação dos modelos publicados pela IBM, foi
utilizada para definir os conjuntos de treino, teste e validação, além de realizar o pré processamento dos dados, incluindo o
processo de normalização.

Vale destacar que, embora \citeonline{Ekambaram2024} sugira o congelamento de todo o \textit{backbone} do modelo, ou seja, 
não atualizar todos os pesos das camadas escondidas durante o ajuste fino, de modo a preservar o conhecimento prévio do modelo,
isso não foi seguido para esse projeto, dado que resultados superiores foram obtidos ao permitir que todas as camadas 
fossem atualizadas. Dessa forma, o modelo é capaz de aprender padrões mais complexos e específicos do conjunto de dados, 
o que pode levar a uma melhor performance nas previsões.

\subsection{Sem Variáveis Exógenas} % ================================================================================ %
Inicialmente, foi implementado o modelo sem considerar as variáveis exógenas, ou seja, apenas com as variáveis dependentes, a 
geração de energia por subsistema e fonte. Essa abordagem, conhecida como \textit{one-shot forecasting}, consiste em aplicar 
um modelo já treinado diretamente sobre a série temporal de interesse para realizar a inferência, sem uma nova etapa de treinamento.

Para esta análise, os dados de 2000 a 2024 foram considerados, devido a capacidade do modelo capturar relações mais complexas. 
O conjunto de dados foi então dividido em frações de treino (70\%), validação (10\%) e teste (20\%), utilizando a função 
\code{prepare\_data\_splits}. Diferentes variantes do modelo foram avaliadas para diferentes amostragens temporais, e os resultados 
serão apresentados no capítulo 5.

O pré-processamento foi realizado pela classe \code{TimeSeriesPreprocessor}, configurada para as variáveis de geração como 
alvo \code{target\_columns}. A classe ajusta os dados para o formato esperado pelo modelo, com janelas de contexto definidas
considerando a variante selecionada, além de aplicar a normalização \code{StandardScaler}, cujos parâmetros são aprendidos 
a partir do conjunto de treino.

Os modelos \code{TinyTimeMixerForPrediction} utilizados nesta etapa foram diferentes variantes pré treinadas, carregadas a partir
de suas respectivas identificações do \textit{Hugging Face}. Apenas as camadas de entrada e saída do modelo foram adaptada 
para corresponder ao número de variáveis de geração.

A previsão é executada através de uma \code{TimeSeriesForecastingPipeline}, que aplica o modelo pré-processado sobre o conjunto 
de teste. Como as previsões são geradas em frequência semanal, um pós-processamento é realizado para agregar os resultados em 
médias mensais, permitindo uma comparação direta com os demais modelos. As métricas $R^2$ e $MSE$ são calculadas sobre os dados 
com a mesma base temporal. Por fim, para cada variável, são gerados e salvos gráficos de dispersão e de série temporal para 
análise visual dos resultados.


\subsection{Com Variáveis Exógenas} % ================================================================================ %
Para a implementação do modelo com variáveis exógenas, faz-se necessário realizar o processo de \textit{fine tuning}, que 
é o processo de atualização dos pesos do modelo pré treinado para adequar-se ao conjunto de dados específico, considerando
a presença de variáveis dependentes e independentes.

Após a consolidação dos dados de geração, carga e ENSO, o conjunto de dados é dividido em frações de treino (70\%), validação 
(10\%) e teste (20\%), utilizando a função \code{prepare\_data\_splits}. A biblioteca \code{tsfm\_public} é novamente empregada 
para o pré-processamento através da classe \code{TimeSeriesPreprocessor}, que desta vez é configurada para tratar tanto as variáveis 
alvo \code{target\_columns} quanto as exógenas \code{control\_columns}, aplicando a normalização \code{StandardScaler} em ambas.

O modelo \code{TinyTimeMixerForPrediction} é carregado, e sua arquitetura é ajustada para o novo conjunto de dados, especificando 
os canais de entrada para as variáveis dependentes e independentes. Uma característica central do modelo é a sua capacidade de 
\textit{mixing}, ou mistura de canais, que permite aprender as interdependências entre as múltiplas séries temporais. Isso é 
habilitado pelos parâmetros \code{decoder\_mode = 'mix\_channel'} e \code{enable\_forecast\_channel\_mixing = True}. Adicionalmente, 
o parâmetro \code{fcm\_prepend\_past = True} é utilizado para que os valores passados das séries também sejam considerados no processo 
de mistura, enriquecendo o contexto disponível para a previsão.

Para o treinamento, a taxa de aprendizado \code{learning\_rate} é definida dinamicamente pela função \code{optimal\_lr\_finder}, 
que busca um valor otimizado para a convergência do modelo. O otimizador utilizado foi o \code{AdamW}, uma variante do otimizador 
Adam que desacopla a regularização de decaimento de peso (\textit{weight decay}) da atualização do gradiente.

O treinamento é gerenciado pela classe \code{Trainer} da biblioteca \code{transformers}, que recebe os hiperparâmetros, 
como o número máximo de épocas, definido em 500, e \textit{batch size} (tamanho do lote) \code{batch\_size = 64}. O tamanho
do lote define o número de amostras que são propagadas através da rede antes da atualização dos pesos. A escolha do tamanho
do lote envolve um compromisso entre a estabilidade da convergência e a capacidade de generalização do modelo. Em tese, 
lotes maiores podem resultar em um treinamento mais rápido, mas podem fazer com que o modelo convirja para um mínimo local, 
enquanto lotes menores podem levar a uma convergência lenta, mas com maior chance de encontrar um mínimo global.

Adicionalmente, o \code{OneCycleLR}, é empregado para variar a taxa de aprendizado de forma cíclica durante o treinamento, 
começando com um valor baixo, aumentando até um máximo e depois diminuindo novamente, o que pode acelerar a convergência.

Uma estratégia de parada antecipada \code{EarlyStoppingCallback} é implementada para monitorar a perda no conjunto de validação 
e interromper o treinamento caso não haja melhora significativa de ao menos 0,001 por 50 épocas consecutivas, 
evitando o sobreajuste (\textit{overfitting}). O modelo com o menor erro de validação é salvo ao final do processo.