\chapter{Metodologia}
\section{Abordagem computacional}

Considerando o escopo do projeto e a grande volume de dados a serem processados, não há outra abordagem viável senão
a utilização de técnicas computacionais. Sendo assim, optou-se por desenvolver o projeto em Python, devido à 
familiaridade do autor com a linguagem e a vasta gama de bibliotecas disponíveis para manipulação de séries históricas, 
análise estatística e aprendizado de máquina. 

Como o computador utilizado possui uma placa de vídeo da AMD e o projeto foi desenvolvido em Windows, não foi possível 
realizar o \textit{offload} do processamento para a GPU, como seria possível com placas NVIDIA nativamente com a 
biblioteca \textit{torch}. Para isso, seria necessário utilizar Linux, em que há suporte da biblioteca para 
a plataforma \textit{Radeon Open Compute} (ROCm). Sendo assim, as etapas do projeto empregando modelos neurais foram 
realizadas utilizando apenas o processador. O sistema utilizado possui um processador AMD Ryzen 5900X e 32 GB de memória
RAM. Todo o processamento intensivo foi realizado em paralelo, utilizando todas as 24 \textit{threads} disponíveis.

\begin{table}[htb]
  \centering
   \IBGEtab{
    \caption{Bibliotecas utilizadas no projeto}
    \label{tab:bibliotecas}
    }{
    \begin{tabular}{llll}
  		  \hline
    	  \textbf{Biblioteca} & \textbf{Descrição} & \textbf{Versão} \\ \hline
        numpy & Cálculos numéricos e manipulação de arrays & 1.26.4 \\
        pandas & Manipulação e análise de dados (DataFrames) & 2.2.3 \\
        requests & Requisições HTTP & 2.32.3 \\
        urllib3 & Gerenciamento de conexões HTTP & 2.2.3 \\
        alive\_progress & Barra de progresso para loops & 3.2.0 \\
        netCDF4 & Leitura de arquivos NetCDF & 1.7.2 \\
        cdsapi & API para download de dados do ECMWF & 0.7.5 \\
        geopandas & Manipulação de dados geoespaciais & 1.0.1 \\
        matplotlib & Visualização de dados & 3.9.2 \\
        scikit-learn & Aplicação de modelos iniciais & 1.5.2 \\
        scipy & Ferramentas e algoritmos matemáticos & 1.14.1 \\
        transformers & Modelos Neurais Pré-treinadis & 4.52.3 \\
        torch & Processamento de Redes Neurais & 2.7.0 \\
        \hline
    \end{tabular}
    }{
      \fonte{o autor.}}
\end{table}

Além disso, toda a base de código foi desenvolvida em Python versão 3.12 e está disponível em um repositório público no
GitHub. O projeto foi organizado em módulos, cada um responsável por uma etapa do processo, desde a obtenção dos dados 
até a implementação dos modelos de aprendizado de máquina. A tabela \ref{tab:bibliotecas} mostra as bibliotecas 
utilizadas no projeto, suas finalidades e versões.

\section{Obtenção e pré-processamento dos dados}
\subsection{Séries históricas do ONS}

A primeira etapa do projeto consiste na consolidação das séries históricas de geração, carga e variáveis hidrológicas,
que são disponibilizadas publicamente no portal Dados Abertos do ONS, a partir do ano 2000. As séries referentes 
às variáveis hidrológicas são disponibilizadas em base diária, e os dados de geração e carga são disponibilizados em 
base horária.

Os dados de geração são disponibilizados em Mega Watt médio (MWmed) por fonte de energia, subsistema, estado, 
modalidade de operação, entre outras variáveis. Os dados de carga também são disponibilizados em MWmed e contêm 
informações sobre a carga em cada subsistema do SIN.

Para as séries de geração, os dados de 2000 a 2021 são agrupados pelos respectivos anos, e a partir de 2022,
as informações estão agrupadas em arquivos por mês e ano. Para as séries de carga, os dados são disponibilizados por
ano. Como o ONS não disponibiliza \textit{Aplication Programming Interface} (API) para a obtenção dos dados diretamente, foi 
necessário uma outra abordagem, a fim de evitar o download manual dos dados. 

Após identificar o padrão de nomenclatura utilizado pelo ONS para os arquivos, foram desenvolvidas as funções 
\textit{GetGeracao}, \textit{GetCarga} e \textit{GetVazao} para obtenção dos dados de geração, carga e hidrológicos,
respectivamente. Os arquivos foram baixados por meio de requisições HTTP, utilizando a biblioteca \textit{requests} e
a biblioteca \textit{urllib3} para gerenciar as conexões. Além disso, as funções realizam o download dos arquivos em
paralelo, utilizando todas as threads disponíveis do sistema. Ao todo, cerca de 10 GB de dados em arquivos Comma 
Separated Values (CSV) foram consolidados.

A fim de obter uma amostragem representativa e suficiente para aplicação dos modelos computacionais, optou-se por 
fazer uma reamostragem dos dados em base horária para a base diária. Sendo assim, considerando o período de 2000 
a 2024, foram consolidados ao todo 9132 amostras diárias para cada variável. Caso fosse considerada a amostragem mensal,
o número de amostras seria de apenas 300, o que poderia não ser suficiente para a aplicação dos modelos. Ainda assim, 
análises em base mensal poderão ser realizadas posteriormente, através de outro processo de reamostragem.

O código \ref{listing:get_geracao} mostra a função \textit{GetGeracao}, que recebe como argumentos a lista de
anos de interesse e a URL base em que os dados estão hospedados. A mesma abordagem foi utilizada para as funções
\textit{GetCarga} e \textit{GetVazao}.

\begin{codigo}[htbp]
\caption{Nome do seu código}
\begin{minted}{python}
def GetGeracao(anos, url, dir):
  def DownloadAno(ano):
      if ano < 2022:
          arq = f'GERACAO_USINA-2_{ano}.csv'
          if not os.path.exists(f'{dir}{arq}.csv'):
              response = requests.get(f'{url}{arq}.csv'', stream = True, verify = True)
              if response.status_code == 200:
                  with open(f'{dir}{arq}{ano}.csv', 'wb') as file:
                      for chunk in response.iter_content(chunk_size = 10240000):
                          if chunk:
                              file.write(chunk)
                              file.flush()
                  return True
              else: return False
      else:
          for mes in np.arange(1, 13, 1):
              if mes < 10: mes = f'0{mes}'
              dir = 'Exportado/ONS/'
              arq = 'GERACAO_USINA-2_'
              url = f'{base_url}{arq}{ano}_{mes}.csv'
              if not os.path.exists(f'{dir}{arq}{ano}_{mes}.csv'):
                  response = requests.get(url, stream = True, verify = True)
                  if response.status_code == 200:
                      with open(f'{dir}{arq}{ano}_{mes}.csv', 'wb') as file:
                          for chunk in response.iter_content(chunk_size = 10240000):
                              if chunk:
                                  file.write(chunk)
                                  file.flush()
                      concluido = True
                  else:
                      concluido = False
          return concluido
  with ThreadPoolExecutor(max_workers = None) as executor:
    executor.map(DownloadAno, anos)
\end{minted}
\end{codigo}

\begin{table}[htb]
  \centering
   \IBGEtab{
    \caption{Parâmetros dos dados de geração}
    \label{tab:geracao}
    }{
      \begin{tabular}{llll}
          \hline
          \textbf{Parâmetro} & \textbf{Descrição} & \textbf{Tipo} \\ \hline
          din\_instante & Instante de aferição & Datetime\\
          nom\_subsistema & Subsistema da usina & String\\
          id\_estado & Estado onde a usina está localizada & String\\ 
          nom\_tipousina & Tipo de usina & String\\
          nom\_tipocombustivel & Tipo de combustível & String\\
          nom\_usina & Nome da usina & String\\
          val\_geracao & Geração de energia (MWmed) & Float\\ \hline
      \end{tabular}
    } {
      \fonte{\citeonline{pen2024}}
      \nota{Variáveis não utilizadas foram omitidas.}
    }
\end{table}

\begin{table}[htb]
  \centering
   \IBGEtab{
    \caption{Parâmetros dos dados de carga}
    \label{tab:carga}
    }{
    \begin{tabular}{llll}
  		  \hline
    	  \textbf{Parâmetro} & \textbf{Descrição} & \textbf{Tipo} \\ \hline
        din\_instante & Instante de aferição & Datetime\\
        nom\_subsistema & Subsistema da usina & String\\
        val\_cargaenergiahomwmed & Carga de energia (MWmed) & Float\\ \hline
    \end{tabular}
    }{
      \fonte{\citeonline{pen2024}}}
\end{table}

Verifica-se, a partir da tabela \ref{tab:geracao}, que os dados de geração contém informações que permitem uma análise 
detalhada da operação do SIN em diferentes níveis de granulidade. Dessa maneira, possíveis impactos em diferentes 
escalas geográficas e temporais poderão ser avaliados. Por outro lado, a tabela \ref{tab:carga} mostra que os dados de
carga não possuem a mesma granulidade que os dados de geração, limitando a análise a nível de subsistema, conforme a
figura \ref{fig:subsistemas_brasil}.


\subsection{Séries históricas de variáveis climatológicas}
A fim de maximizar o potencial de análise do projeto, é essencial considerar um grande número de dados climatológicos, 
como anomalis de temperatura, precipitação, pressão, radiação solar, entre outros. No entanto, a obtenção de todos esses
dados em base diária só é possível através do portal Climate Data Store (CDS) do ECMWF (European Centre for Medium-Range
 Weather Forecasts), através do ERA5, que é um reanálise climática de alta resolução espacial e temporal. \cite{C3S2024}

Embora os dados do ERA5 possam ser obtidos através da API do CDS, sendo possível especificar a escala temporal e a área
geográfica de interesse, há um limite de requisições por usuário e uma file de espera para o processamento das 
requisições. Sendo assim, apenas os dados de temperatura da superfície do mar (TSM) e precipitação das regiões do ENSO 
foram obtidos em base diária. O código \ref{lst:get_era5} mostra como as solicitações à API do CDS foram realizadas,
utilizando a biblioteca \textit{cdsapi}.

\begin{minted}{python}
import cdsapi

dataset = "derived-era5-single-levels-daily-statistics"
request = {
    "product_type": "reanalysis",
    "variable": ["sea_surface_temperature"],
    "year": "2024",
    "month": [
        "01", "02", "03",
        "04", "05", "06",
        "07", "08", "09",
        "10", "11", "12"
    ],
    "day": [
        "01", "02", "03",
        "04", "05", "06",
        "07", "08", "09",
        "10", "11", "12",
        "13", "14", "15",
        "16", "17", "18",
        "19", "20", "21",
        "22", "23", "24",
        "25", "26", "27",
        "28", "29", "30",
        "31"
    ],
    "daily_statistic": "daily_mean",
    "time_zone": "utc+00:00",
    "frequency": "1_hourly"
}

client = cdsapi.Client()
client.retrieve(dataset, request).download()

\end{minted}

Os dados do ECMWF foram obtidos através da API do CDS (Climate Data Store), que permite o download de dados 
meteorológicos de alta resolução, sendo possível especificar a escala temporal e a área de interesse, especificando as 
coordenadas geográficas. Os dados do NOAA foram obtidos através de requisições HTTP.

É importante ressaltar que, embora a maioria dos dados climatológicos seja disponibilizada em base mensal, alguns
possuem escala temporal diária, como alguns dados do ECMWF. Considerando que os dados de energia são disponibilizados em
base horária, diferentes análises poderão ser realizadas, considerando as escalas temporais diária e mensal. Para isso,
durante a etapa de pré-processamento dos dados, será realizada a consolidação dos dados de geração e carga de modo que
estejam na mesma escala temporal dos dados climatológicos.

\begin{figure}[!ht]
	\IBGEtab{\caption{Subsistemas do SIN}
			 \label{fig:subsistemas_brasil}}
	{\includesvg[scale=1]{figuras/subsistemas_brasil}}
	{\fonte{o autor.}}
\end{figure}

Além disso, nas próximas etapas do projeto, será realizado a seleção das variáveis climatológicas mais relevantes para
a análise, considerando a correlação com a geração de energia. Para isso, serão utilizadas técnicas de \textit{feature
selection} e redução de dimensionalidade, como a análise de componentes principais. A tabela \ref{tab:variaveis_clima}
mostra as variáveis climatológicas utilizadas no projeto.

\subsection{Pré-processamento dos dados}


\section{Análise exploratória dos dados}


\section{Implementação dos modelos de regressão}


\subsection{Feature selection}


\subsection{Análise de componentes principais}


\subsection{Avaliação dos modelos de regressão}


\section{Implementação dos modelos de aprendizado de máquina}


\subsection{Feature selection}


\subsection{Análise de componentes principais}


\subsection{Avaliação dos modelos de aprendizado de máquina}
